# selfattn
Implementation of self-attention methods


Includes:

Self-attention with single head

Self-attention with multiple heads, with/out random-overlapping channel splits

Self-attention stem from paper titled 'Stand-Alone Self-Attention in Vision Models'

Group attention based on paper titled 'Twins: Revisiting the Design of Spatial Attention in Vision Transformers'

BERT attention for visual transformers w/o recurrent layers


Content subject to change in future.
